{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf==4.0.2 langchain==0.1.9 ollama==0.1.6 chromadb==0.4.23 PyPDF2==3.0.1 jupyterlab==4.1.2\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "from subprocess import Popen\n",
        "Popen(\"ollama serve\".split())"
      ],
      "metadata": {
        "id": "0nO2KFpnGdiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama pull llama3\n",
        "!ollama pull nomic-embed-text"
      ],
      "metadata": {
        "id": "iW7gefjeu926"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import hub\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "from langchain.callbacks.manager import CallbackManager\n",
        "from langchain.llms import Ollama\n",
        "from langchain.embeddings.ollama import OllamaEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "import os\n",
        "import time\n",
        "filepath = \"file.pdf\"\n",
        "localmodel = \"llama3\"\n",
        "embeding = \"nomic-embed-text\"\n",
        "\n",
        "loader = PyPDFLoader(filepath)\n",
        "data = loader.load()\n",
        "\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=100)\n",
        "all_splits = text_splitter.split_documents(data)\n",
        "persist_directory = 'data'"
      ],
      "metadata": {
        "id": "dgnEgQhdAah0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore = Chroma.from_documents(all_splits, embedding=OllamaEmbeddings(model=embeding), persist_directory=persist_directory)"
      ],
      "metadata": {
        "id": "IQOuP43aZQ0g"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = Ollama(base_url=\"http://localhost:11434\", model=localmodel, verbose=True,\n",
        "             callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]))\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "template = \"\"\" Answer the question based only on the following\n",
        "context: {context}\n",
        "\n",
        "User: {question}\n",
        "Chatbot:\n",
        "\"\"\"\n",
        "prompt = PromptTemplate(input_variables=[\"context\",\"question\"], template=template,)\n",
        "#memory = ConversationBufferMemory(memory_key=\"history\", return_messages=True, input_key=\"question\")\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type='stuff', retriever=retriever, verbose=False,\n",
        "                                       chain_type_kwargs={\n",
        "                                           \"verbose\": True,\n",
        "                                           \"prompt\": prompt,\n",
        "                                           #\"memory\": memory,\n",
        "                                       })"
      ],
      "metadata": {
        "id": "D4lT38LvJQQH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"question\"\n",
        "res = qa_chain.invoke({\"query\": query})\n",
        "print(res['result'])"
      ],
      "metadata": {
        "id": "OM9Kae3vK_Ku"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}