{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0nO2KFpnGdiG"
      },
      "outputs": [],
      "source": [
        "!pip install pypdf==4.0.2 langchain==0.1.9 ollama==0.1.6 chromadb==0.4.23 PyPDF2==3.0.1 jupyterlab==4.1.2 streamlit\n",
        "!curl -L https://ollama.com/download/ollama-linux-amd64.tgz -o ollama-linux-amd64.tgz\n",
        "!tar -C /usr -xzf ollama-linux-amd64.tgz\n",
        "from subprocess import Popen\n",
        "Popen(\"ollama serve\".split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "iW7gefjeu926"
      },
      "outputs": [],
      "source": [
        "!ollama pull llama3.1\n",
        "!ollama pull nomic-embed-text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "dgnEgQhdAah0"
      },
      "outputs": [],
      "source": [
        "from langchain import hub\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "from langchain.callbacks.manager import CallbackManager\n",
        "from langchain.llms import Ollama\n",
        "from langchain.embeddings.ollama import OllamaEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "import os\n",
        "import time\n",
        "from google.colab import files\n",
        "mainfile = files.upload()\n",
        "mf = list(mainfile)[0]\n",
        "filepath = mf\n",
        "localmodel = \"llama3.1\"\n",
        "embeding = \"nomic-embed-text\"\n",
        "\n",
        "loader = PyPDFLoader(filepath)\n",
        "data = loader.load()\n",
        "\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=100)\n",
        "all_splits = text_splitter.split_documents(data)\n",
        "persist_directory = 'data'\n",
        "\n",
        "vectorstore = Chroma.from_documents(all_splits, embedding=OllamaEmbeddings(model=embeding), persist_directory=persist_directory)\n",
        "llm = Ollama(base_url=\"http://localhost:11434\", model=localmodel, verbose=True,\n",
        "             callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]))\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "template = \"\"\" Answer the question based only on the following\n",
        "context: {context}\n",
        "\n",
        "User: {question}\n",
        "Chatbot:\n",
        "\"\"\"\n",
        "prompt = PromptTemplate(input_variables=[\"context\",\"question\"], template=template,)\n",
        "#memory = ConversationBufferMemory(memory_key=\"history\", return_messages=True, input_key=\"question\")\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type='stuff', retriever=retriever, verbose=False,\n",
        "                                       chain_type_kwargs={\n",
        "                                           #\"verbose\": True,\n",
        "                                           \"prompt\": prompt,\n",
        "                                           #\"memory\": memory,\n",
        "                                       })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OM9Kae3vK_Ku"
      },
      "outputs": [],
      "source": [
        "query = \"\"\n",
        "res = qa_chain.invoke({\"query\": query})\n",
        "print(res['result'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hdQs928yQEvz"
      },
      "outputs": [],
      "source": [
        "from threading import Timer\n",
        "def ff():\n",
        "  !npx localtunnel --port 8501\n",
        "t = Timer(5,ff)\n",
        "t.start()\n",
        "\n",
        "!curl https://loca.lt/mytunnelpassword\n",
        "!streamlit run app.py"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}