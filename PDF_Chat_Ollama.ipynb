{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0nO2KFpnGdiG",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Start Server (Install)\n",
        "from IPython.display import clear_output\n",
        "!pip install pypdf langchain langchain-community langchain-chroma langchain-ollama ollama chromadb PyPDF2 jupyterlab  django\n",
        "!curl -L https://ollama.com/download/ollama-linux-amd64.tgz -o ollama-linux-amd64.tgz\n",
        "!tar -C /usr -xzf ollama-linux-amd64.tgz\n",
        "from subprocess import Popen\n",
        "Popen(\"ollama serve\".split())\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iW7gefjeu926",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Start Server (Download Model)\n",
        "!ollama pull llama3.2\n",
        "!ollama pull nomic-embed-text\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgnEgQhdAah0",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Start Server (Initialize)\n",
        "from langchain import hub\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "from langchain.callbacks.manager import CallbackManager\n",
        "from langchain_ollama import OllamaLLM\n",
        "from langchain_ollama import OllamaEmbeddings\n",
        "from langchain_chroma import Chroma\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "import os\n",
        "import time\n",
        "\n",
        "def pdflrn(file,path,embedmodel):\n",
        "  loader = PyPDFLoader(file)\n",
        "  data = loader.load()\n",
        "\n",
        "\n",
        "  text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=100)\n",
        "  all_splits = text_splitter.split_documents(data)\n",
        "\n",
        "\n",
        "  vectorstore = Chroma.from_documents(all_splits, embedding=OllamaEmbeddings(model=embedmodel), persist_directory=path)\n",
        "  return True\n",
        "\n",
        "def pdfres(query,path,model,embedmodel):\n",
        "  vectorstore = Chroma(embedding_function=OllamaEmbeddings(model=embedmodel), persist_directory=path)\n",
        "  llm = OllamaLLM(base_url=\"http://localhost:11434\", model=model, verbose=True,\n",
        "              callbacks=[StreamingStdOutCallbackHandler()])\n",
        "  retriever = vectorstore.as_retriever()\n",
        "\n",
        "  template = \"\"\" Answer the question based only on the following\n",
        "  context: {context}\n",
        "\n",
        "  User: {question}\n",
        "  Chatbot:\n",
        "  \"\"\"\n",
        "  prompt = PromptTemplate(input_variables=[\"context\",\"question\"], template=template,)\n",
        "  #memory = ConversationBufferMemory(memory_key=\"history\", return_messages=True, input_key=\"question\")\n",
        "\n",
        "  qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type='stuff', retriever=retriever, verbose=False,\n",
        "                                        chain_type_kwargs={\n",
        "                                            #\"verbose\": True,\n",
        "                                            \"prompt\": prompt,\n",
        "                                            #\"memory\": memory,\n",
        "                                        })\n",
        "  res = qa_chain.invoke({\"query\": query})\n",
        "  return res[\"result\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OM9Kae3vK_Ku",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Learn\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "for fn in uploaded:\n",
        "  pdflrn(fn,\"data\",\"nomic-embed-text\")\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Request a query\n",
        "query = \"\" # @param {\"type\":\"string\"}\n",
        "re1 = pdfres(query,\"data\",\"llama3.2\",\"nomic-embed-text\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "tdusiai-0jQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Django LLM\n",
        "!git clone https://github.com/epg900/ollama.git\n",
        "%cd ollama/lamachat/\n",
        "!ssh-keygen -t rsa -f ~/.ssh/id_rsa <<< y\n",
        "clear_output()\n",
        "from threading import Timer\n",
        "def tt():\n",
        "    !ssh srv.us -R 1:localhost:8000 -o StrictHostKeyChecking=no\n",
        "t = Timer(5,tt)\n",
        "t.start()\n",
        "!python manage.py runserver"
      ],
      "metadata": {
        "id": "fb3MoW7ZhJWn",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}