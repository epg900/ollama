{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0nO2KFpnGdiG",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Start Server (STEP 1)\n",
        "from IPython.display import clear_output()\n",
        "!pip install pypdf langchain langchain-community langchain-chroma langchain-ollama ollama chromadb PyPDF2 jupyterlab  django\n",
        "!curl -L https://ollama.com/download/ollama-linux-amd64.tgz -o ollama-linux-amd64.tgz\n",
        "!tar -C /usr -xzf ollama-linux-amd64.tgz\n",
        "from subprocess import Popen\n",
        "Popen(\"ollama serve\".split())\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iW7gefjeu926",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Start Server (STEP 2)\n",
        "!ollama pull llama3.2\n",
        "!ollama pull nomic-embed-text\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgnEgQhdAah0",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Start Server (STEP 3)\n",
        "from langchain import hub\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "from langchain.callbacks.manager import CallbackManager\n",
        "from langchain_ollama import OllamaLLM\n",
        "from langchain_ollama import OllamaEmbeddings\n",
        "from langchain_chroma import Chroma\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "import os\n",
        "import time\n",
        "from google.colab import files\n",
        "\n",
        "persist_directory = 'data1'\n",
        "localmodel = \"llama3.1\"\n",
        "embeding = \"nomic-embed-text\"\n",
        "\n",
        "def pdflrn():\n",
        "  mainfile = files.upload()\n",
        "  mf = list(mainfile)[0]\n",
        "  filepath = mf\n",
        "\n",
        "\n",
        "  loader = PyPDFLoader(filepath)\n",
        "  data = loader.load()\n",
        "\n",
        "\n",
        "  text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=100)\n",
        "  all_splits = text_splitter.split_documents(data)\n",
        "\n",
        "\n",
        "  vectorstore = Chroma.from_documents(all_splits, embedding=OllamaEmbeddings(model=embeding), persist_directory=persist_directory)\n",
        "  return True\n",
        "\n",
        "def pdfres(query):\n",
        "  vectorstore1 = Chroma(embedding_function=OllamaEmbeddings(model=embeding), persist_directory=persist_directory)\n",
        "  llm = OllamaLLM(base_url=\"http://localhost:11434\", model=localmodel, verbose=True,\n",
        "              callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]))\n",
        "  retriever = vectorstore1.as_retriever()\n",
        "\n",
        "  template = \"\"\" Answer the question based only on the following\n",
        "  context: {context}\n",
        "\n",
        "  User: {question}\n",
        "  Chatbot:\n",
        "  \"\"\"\n",
        "  prompt = PromptTemplate(input_variables=[\"context\",\"question\"], template=template,)\n",
        "  #memory = ConversationBufferMemory(memory_key=\"history\", return_messages=True, input_key=\"question\")\n",
        "\n",
        "  qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type='stuff', retriever=retriever, verbose=False,\n",
        "                                        chain_type_kwargs={\n",
        "                                            #\"verbose\": True,\n",
        "                                            \"prompt\": prompt,\n",
        "                                            #\"memory\": memory,\n",
        "                                        })\n",
        "  res = qa_chain.invoke({\"query\": query})\n",
        "  return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OM9Kae3vK_Ku",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Learn and Query\n",
        "query = \"\" # @param {\"type\":\"string\"}\n",
        "pdflrn()\n",
        "clear_output()\n",
        "re1 = pdfres(query)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}